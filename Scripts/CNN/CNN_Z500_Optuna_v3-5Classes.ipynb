{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f074931-56b1-4d97-abf2-dc600525ddb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import glob\n",
    "import pickle\n",
    "import warnings\n",
    "from datetime import datetime, timedelta\n",
    "from itertools import product\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "import cartopy\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature\n",
    "import cartopy.feature as cfeature\n",
    "import cartopy.feature as cf\n",
    "import cartopy.io.shapereader as shpreader\n",
    "import matplotlib as mpl\n",
    "import matplotlib.path as mpath\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "# np.random.seed(42)\n",
    "import pandas as pd\n",
    "import shapely.geometry as sgeom\n",
    "import xarray as xr\n",
    "from scipy import stats\n",
    "from scipy.spatial.distance import cdist\n",
    "from shapely import geometry\n",
    "from sklearn import metrics\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# import cluster_analysis, narm_analysis, som_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a290e62-b693-402b-8129-7f4ab36d9e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# tf.test.is_gpu_available(\n",
    "#     cuda_only=False, min_cuda_compute_capability=None\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68e7ee19-07c8-4499-a052-6b989d6b3329",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-16 12:59:19.349676: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential,Model\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, AveragePooling2D, Dropout, BatchNormalization,SpatialDropout2D\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.layers import ReLU\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "\n",
    "# import visualkeras\n",
    "# import tensorflow as tf\n",
    "\n",
    "# ## GLOBAL SEED ##    \n",
    "# np.random.seed(42)\n",
    "# tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4af67a6e-4f1e-42fe-8f23-1057214c0863",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/glade/work/jhayron/conda-envs/cnn_wr/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "from optuna.integration import TFKerasPruningCallback\n",
    "\n",
    "optuna.logging.set_verbosity(optuna.logging.WARN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73da05ac-ef73-41f3-9efd-7f1743760936",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90aee4bc-9be4-458e-b3c4-f637b16a7016",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Weather regimes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a40047a6-fca9-4719-9b9c-0d9c0b814f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "week1_wr = pd.read_csv('/glade/work/jhayron/Weather_Regimes/weekly_wr/week1_wr_v3.csv',\\\n",
    "                      index_col = 0, parse_dates = True)\n",
    "week2_wr = pd.read_csv('/glade/work/jhayron/Weather_Regimes/weekly_wr/week2_wr_v3.csv',\\\n",
    "                      index_col = 0, parse_dates = True)\n",
    "week3_wr = pd.read_csv('/glade/work/jhayron/Weather_Regimes/weekly_wr/week3_wr_v3.csv',\\\n",
    "                      index_col = 0, parse_dates = True)\n",
    "week4_wr = pd.read_csv('/glade/work/jhayron/Weather_Regimes/weekly_wr/week4_wr_v3.csv',\\\n",
    "                      index_col = 0, parse_dates = True)\n",
    "week5_wr = pd.read_csv('/glade/work/jhayron/Weather_Regimes/weekly_wr/week5_wr_v3.csv',\\\n",
    "                      index_col = 0, parse_dates = True)\n",
    "week6_wr = pd.read_csv('/glade/work/jhayron/Weather_Regimes/weekly_wr/week6_wr_v3.csv',\\\n",
    "                      index_col = 0, parse_dates = True)\n",
    "week7_wr = pd.read_csv('/glade/work/jhayron/Weather_Regimes/weekly_wr/week7_wr_v3.csv',\\\n",
    "                      index_col = 0, parse_dates = True)\n",
    "week8_wr = pd.read_csv('/glade/work/jhayron/Weather_Regimes/weekly_wr/week8_wr_v3.csv',\\\n",
    "                      index_col = 0, parse_dates = True)\n",
    "week9_wr = pd.read_csv('/glade/work/jhayron/Weather_Regimes/weekly_wr/week9_wr_v3.csv',\\\n",
    "                      index_col = 0, parse_dates = True)\n",
    "\n",
    "df_wr = pd.concat([week1_wr,week2_wr,week3_wr,week4_wr,week5_wr,week6_wr,week7_wr,week8_wr,week9_wr],axis=1)\n",
    "df_wr.columns = ['week1','week2','week3','week4','week5','week6','week7','week8','week9']\n",
    "\n",
    "df_wr_2 = pd.read_csv('/glade/work/jhayron/Weather_Regimes/weekly_wr/weekly_wr_mean_geop_v3.csv',\n",
    "                     index_col=0,parse_dates=True)\n",
    "\n",
    "df_wr_2 = df_wr_2.dropna()\n",
    "df_wr = df_wr.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a71f5dfa-fe2c-4379-a0e7-dba8938ecdea",
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_wr = pd.read_csv('/glade/work/jhayron/Weather_Regimes/weekly_wr/weekly_distance_mean_geop_v3.csv',\n",
    "                         index_col = 0,parse_dates=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9cf6cebd-00be-4c41-a8db-66887c72cbef",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wr_2[distance_wr>=np.mean(distance_wr.iloc[:,0]) + 1.5 * np.std(distance_wr.iloc[:,0])] = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e404c3d-4968-4204-b0ed-fe95bfb6d6ae",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Input Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "beb3164d-10c0-4073-ac3a-4f8d4634b5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "variables = ['z500']\n",
    "name_var = ['z500']\n",
    "units = ['m2/s2']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "caa0c0e0-15fa-47a6-8117-d6f2a8313107",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dic_vars = {}\n",
    "for var_short, variable,unit in zip(name_var,variables,units):\n",
    "# for var_short, variable,unit in zip(['sst'],['sst'],['K']):\n",
    "    path_w_anoms = '/glade/work/jhayron/Weather_Regimes/weekly_anomalies/'\n",
    "    week1_anoms = xr.open_dataset(f'{path_w_anoms}week1_{variable}_anoms_v3.nc')\n",
    "    # week1_anoms = week1_anoms.sel(time=df_wr_2.index)\n",
    "    if variable=='z500':\n",
    "        week1_anoms = week1_anoms.where(week1_anoms.lat>-30,drop=True)\n",
    "    # week1_anoms = week1_anoms.sel(time=df_wr.index)\n",
    "    week1_anoms = week1_anoms.sel(time=df_wr_2.index)\n",
    "    dic_vars[variable] = week1_anoms\n",
    "    \n",
    "    ##########PLOT#####################\n",
    "#     fig = plt.figure(figsize=(9,7))\n",
    "#     ax = fig.add_subplot(111,projection=ccrs.PlateCarree(central_longitude=-90+360))\n",
    "    \n",
    "#     #     vmax = np.round(np.percentile(abs(week1_anoms[f'{var_short}_anomalies'].values[0]),100),0)\n",
    "#     #     vmin = -np.round(np.percentile(abs(week1_anoms[f'{var_short}_anomalies'].values[0]),100),0)\n",
    "    \n",
    "#     vmax = np.round(np.percentile(abs(week1_anoms[f'{var_short}_anomalies'].values[0]),100),0)\n",
    "#     vmin = -np.round(np.percentile(abs(week1_anoms[f'{var_short}_anomalies'].values[0]),100),0)\n",
    "#     # print(vmax,vmin)\n",
    "#     cs = plt.pcolormesh(week1_anoms.lon,week1_anoms.lat,\\\n",
    "#         week1_anoms[f'{var_short}_anomalies'].values[0],cmap='seismic',\n",
    "#         transform=ccrs.PlateCarree(),vmin=vmin,vmax=vmax)\n",
    "#     ax.coastlines(resolution='110m', color='k', linewidth=0.75, zorder=10)\n",
    "#     ax.margins(x=0, y=0)\n",
    "\n",
    "#     # ax.set_extent([-179, 179, 10, 90], crs=ccrs.PlateCarree())\n",
    "#     if variable!='st':\n",
    "#         plt.title(variable.upper().replace('_','-'))\n",
    "#     else:\n",
    "#         plt.title(variable.upper().replace('_','-').replace('ST','TS'))\n",
    "#     if 'region' in variable:\n",
    "#         cbar_ax = fig.add_axes([0.25, 0.19, 0.5, 0.0175])\n",
    "#     else:\n",
    "#         cbar_ax = fig.add_axes([0.25, 0.3, 0.5, 0.0175])\n",
    "#     # ticks_1 = [-80, -40, 0, 40, 80]\n",
    "#     cbar = fig.colorbar(cs, cax=cbar_ax,\n",
    "#                         orientation='horizontal', extend='both')\n",
    "#     cbar.ax.tick_params(labelsize=14)\n",
    "#     cbar.set_label(unit, fontsize=14)\n",
    "#     plt.savefig(f'/glade/u/home/jhayron/WeatherRegimes/Figures/MapsVariables/{variable}_anomalies_v2.png',bbox_inches='tight')\n",
    "#     plt.close('all')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b09832-3831-4dc7-aeb2-6b7b65e854e8",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Build models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472b482e-df08-4a1a-8d3d-39e9d75a8717",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2458a532-676a-42e8-b19b-ee3e630ee4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(ks,ps,type_pooling,stc,stp,do,bn,md,nfilters,activation):\n",
    "    num_classes = 5\n",
    "    \n",
    "    if activation == 'LeakyReLU':\n",
    "        activation_conv= LeakyReLU()\n",
    "    elif activation == 'ReLU':\n",
    "        activation_conv= ReLU()\n",
    "        \n",
    "    padding_type = 'same'\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Conv2D(nfilters, kernel_size=(ks, ks),activation=activation_conv,\n",
    "        input_shape=X_train.shape[1:],padding=padding_type,strides=stc))\n",
    "    \n",
    "    if type_pooling == 'Max':\n",
    "        model.add(MaxPooling2D((ps, ps),padding=padding_type,strides=stp))\n",
    "    elif type_pooling == 'Average':\n",
    "        model.add(AveragePooling2D((ps, ps),padding=padding_type,strides=stp))\n",
    "        \n",
    "    model.add(Dropout(do))\n",
    "    if bn==True:\n",
    "        model.add(BatchNormalization())\n",
    "        \n",
    "    model.add(Conv2D(nfilters*2, (ks, ks), activation=activation_conv,padding=padding_type,strides=stc))\n",
    "    \n",
    "    if type_pooling == 'Max':\n",
    "        model.add(MaxPooling2D((ps, ps),padding=padding_type,strides=stp))\n",
    "    elif type_pooling == 'Average':\n",
    "        model.add(AveragePooling2D((ps, ps),padding=padding_type,strides=stp))\n",
    "        \n",
    "    model.add(SpatialDropout2D(do))\n",
    "    if bn==True:\n",
    "        model.add(BatchNormalization())\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(num_classes*md*md, activation=activation_conv))\n",
    "    model.add(Dense(num_classes*md, activation=activation_conv))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    model.compile(loss=keras.losses.categorical_crossentropy, \n",
    "        optimizer=keras.optimizers.Adam(lr=0.001),metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c04e4602-7006-4b85-8bdb-1af5d2b85a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Objective(object):\n",
    "    def __init__(self, X_train, y_train, X_val, y_val,\n",
    "                 path_models, variable, week, d_class_weights):\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.X_val = X_val\n",
    "        self.y_val = y_val\n",
    "        self.path_models = path_models\n",
    "        self.variable = variable\n",
    "        self.week = week\n",
    " \n",
    "    def __call__(self, trial):    \n",
    "        keras.backend.clear_session()\n",
    "                \n",
    "        ks = trial.suggest_categorical('ks',[3,5,7,9,11])\n",
    "        ps = trial.suggest_categorical('ps',[2,4,6,8])\n",
    "        type_pooling = trial.suggest_categorical('type_pooling',['Average','Max'])\n",
    "        stc = trial.suggest_categorical('stc',[1,2,3,4])\n",
    "        stp = trial.suggest_categorical('stp',[1,2,3,4])\n",
    "        do = trial.suggest_categorical('do',[0.3,0.4,0.5])\n",
    "        bn = trial.suggest_categorical('bn',[True,False])\n",
    "        md = trial.suggest_categorical('md',[2,4,8,16,32])\n",
    "        nfilters = trial.suggest_categorical('nfilters',[4,8,16,32])\n",
    "        activation = trial.suggest_categorical('activation',['LeakyReLU','ReLU'])\n",
    "        bs = trial.suggest_categorical('bs',[16,32,64])\n",
    "        \n",
    "        dict_params = {'ks':ks,\n",
    "                       'ps':ps,\n",
    "                       'type_pooling':type_pooling,\n",
    "                       'stc':stc,\n",
    "                       'stp':stp,\n",
    "                       'do':do,\n",
    "                       'bn':bn,\n",
    "                       'md':md,\n",
    "                       'nfilters':nfilters,\n",
    "                       'activation':activation,\n",
    "                       # 'lr':lr,\n",
    "                       'bs':bs}\n",
    "                                              \n",
    "        # instantiate and compile model\n",
    "        cnn_model = create_model(dict_params['ks'],\n",
    "                                 dict_params['ps'],\n",
    "                                 dict_params['type_pooling'],\n",
    "                                 dict_params['stc'],\n",
    "                                 dict_params['stp'],\n",
    "                                 dict_params['do'],\n",
    "                                 dict_params['bn'],\n",
    "                                 dict_params['md'],\n",
    "                                 dict_params['nfilters'],\n",
    "                                 dict_params['activation'],\n",
    "                                 # dict_params['lr'],\n",
    "                                )\n",
    "        \n",
    "        epochs = 50\n",
    "        earlystop = EarlyStopping(monitor='val_loss', patience=5)\n",
    "        try:\n",
    "            os.mkdir(f'{self.path_models}{self.variable}')\n",
    "        except: pass\n",
    "        filepath = f'{self.path_models}{self.variable}/model_{self.week}_v7.h5'\n",
    "        checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=0, save_best_only=True, \n",
    "                                     mode='auto',save_weights_only=False)\n",
    "        h = cnn_model.fit(self.X_train, self.y_train, batch_size=dict_params['bs'],\\\n",
    "            epochs=epochs,verbose=0,validation_data=(self.X_val, self.y_val), \\\n",
    "            callbacks=[checkpoint,earlystop],class_weight = d_class_weights) #TFKerasPruningCallback(trial, \"val_loss\")\n",
    "\n",
    "        validation_loss = np.min(h.history['val_loss'])\n",
    "        return validation_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5d68fabd-1bdb-4a53-9288-ea70e2ff660f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logging_callback(study, frozen_trial):\n",
    "    previous_best_value = study.user_attrs.get(\"previous_best_value\", None)\n",
    "    if previous_best_value != study.best_value:\n",
    "        study.set_user_attr(\"previous_best_value\", study.best_value)\n",
    "        print(\n",
    "            \"Trial {} finished with best value: {} and parameters: {}. \".format(\n",
    "            frozen_trial.number,\n",
    "            frozen_trial.value,\n",
    "            frozen_trial.params,\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5daa2338-fc25-4638-b503-36e7da6b6ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_models = '/glade/work/jhayron/Weather_Regimes/models/CNN/weights_variables_v8/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1c24ab-06d0-42c5-91a2-fa4a70e28111",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************************************************************************************\n",
      "z500\n",
      "********************************************************************************************\n",
      "week1\n",
      "Trial 0 finished with best value: 0.5757694840431213 and parameters: {'ks': 9, 'ps': 4, 'type_pooling': 'Average', 'stc': 2, 'stp': 1, 'do': 0.5, 'bn': False, 'md': 2, 'nfilters': 4, 'activation': 'LeakyReLU', 'bs': 16}. \n",
      "Trial 5 finished with best value: 0.5279024839401245 and parameters: {'ks': 9, 'ps': 6, 'type_pooling': 'Average', 'stc': 1, 'stp': 4, 'do': 0.4, 'bn': True, 'md': 4, 'nfilters': 32, 'activation': 'LeakyReLU', 'bs': 32}. \n",
      "Trial 18 finished with best value: 0.5117121338844299 and parameters: {'ks': 3, 'ps': 4, 'type_pooling': 'Max', 'stc': 1, 'stp': 4, 'do': 0.4, 'bn': False, 'md': 16, 'nfilters': 32, 'activation': 'LeakyReLU', 'bs': 16}. \n",
      "Trial 28 finished with best value: 0.4995255768299103 and parameters: {'ks': 9, 'ps': 4, 'type_pooling': 'Average', 'stc': 1, 'stp': 4, 'do': 0.4, 'bn': False, 'md': 4, 'nfilters': 32, 'activation': 'LeakyReLU', 'bs': 32}. \n"
     ]
    }
   ],
   "source": [
    "dic_metrics = {}\n",
    "\n",
    "for var_short, variable in zip(name_var,variables):\n",
    "    print('********************************************************************************************')\n",
    "    print(variable)\n",
    "    print('********************************************************************************************')\n",
    "    loss_weeks_model = []\n",
    "    loss_weeks_persistence = []\n",
    "    acc_weeks_model = []\n",
    "    acc_weeks_persistence = []\n",
    "\n",
    "    for week in ['week1','week2','week3']:\n",
    "        print(week)\n",
    "        #### ORGANIZE DATA ####\n",
    "        week_output_wr = df_wr_2[week].values.astype(int)\n",
    "        # Make Y categorical\n",
    "        serie_wr_categorical = to_categorical(week_output_wr,num_classes=5)\n",
    "        week1_anoms = copy.deepcopy(dic_vars[variable])\n",
    "        \n",
    "        # # Scale by min-max\n",
    "        Min = week1_anoms[f'{var_short}_anomalies'].min(dim='time')\n",
    "        Max = week1_anoms[f'{var_short}_anomalies'].max(dim='time')\n",
    "        scaled_x = (week1_anoms[f'{var_short}_anomalies']) / (Max - Min)\n",
    "\n",
    "        indices = np.arange(len(serie_wr_categorical))\n",
    "        #Reshape X\n",
    "        scaled_x = scaled_x.data.reshape(-1, scaled_x.shape[1],scaled_x.shape[2], 1)\n",
    "\n",
    "        indices_train = np.where(df_wr_2.week2.index.year<=2001)[0]\n",
    "        indices_val = np.where((df_wr_2.week2.index.year>2001)&(df_wr_2.week2.index.year<=2010))[0]\n",
    "        indices_test = np.where(df_wr_2.week2.index.year>2010)[0]\n",
    "\n",
    "        X_test = scaled_x[indices_test]\n",
    "        y_test = serie_wr_categorical[indices_test]\n",
    "\n",
    "        X_train = scaled_x[indices_train]\n",
    "        y_train = serie_wr_categorical[indices_train]\n",
    "\n",
    "        X_val = scaled_x[indices_val]\n",
    "        y_val = serie_wr_categorical[indices_val]\n",
    "\n",
    "        wr_persistence = df_wr_2.week1.values.astype(int)[indices_test]\n",
    "        serie_wr_persistence_categorical = to_categorical(wr_persistence)\n",
    "        \n",
    "        y_train_integers = np.argmax(y_train, axis=1)\n",
    "        class_weights = compute_class_weight(class_weight='balanced',classes=np.unique(y_train_integers),\n",
    "                                             y = y_train_integers)\n",
    "        d_class_weights = dict(enumerate(class_weights))\n",
    "        \n",
    "        #### TRAIN ####\n",
    "        # def print_best_callback(study, trial):\n",
    "        #     print(f\"Best value: {study.best_value}, Best params: {study.best_trial.params}\")\n",
    "\n",
    "        optimizer_direction = 'minimize'\n",
    "        number_of_random_points = 25  # random searches to start opt process\n",
    "        maximum_time = 0.5*60*60  # seconds\n",
    "        objective = Objective(X_train,y_train,X_val,y_val,path_models,variable,week,d_class_weights)\n",
    "\n",
    "        # optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "        study = optuna.create_study(direction=optimizer_direction,\n",
    "                sampler=TPESampler(n_startup_trials=number_of_random_points))\n",
    "\n",
    "        study.optimize(objective, timeout=maximum_time, gc_after_trial=True,callbacks=[logging_callback])\n",
    "        \n",
    "        results_directory = f'/glade/work/jhayron/Weather_Regimes/models/CNN/results_optuna/{week}/'\n",
    "        # save results\n",
    "        df_results = study.trials_dataframe()\n",
    "        df_results.to_pickle(results_directory + 'df_optuna_results_v3.pkl')\n",
    "        df_results.to_csv(results_directory + 'df_optuna_results_v3.csv')\n",
    "        #save study\n",
    "        joblib.dump(study, results_directory + 'optuna_study_v3.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e446db-d8d1-468a-9edd-7fbcc63baeac",
   "metadata": {},
   "source": [
    "# Train and save final models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f90479a6-bdce-41c7-a9c7-dcec190c570c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(ks,ps,type_pooling,stc,stp,do,bn,md,nfilters,activation):\n",
    "    num_classes = 5\n",
    "    \n",
    "    if activation == 'LeakyReLU':\n",
    "        activation_conv= LeakyReLU()\n",
    "    elif activation == 'ReLU':\n",
    "        activation_conv= ReLU()\n",
    "        \n",
    "    padding_type = 'same'\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Conv2D(nfilters, kernel_size=(ks, ks),activation=activation_conv,\n",
    "        input_shape=X_train.shape[1:],padding=padding_type,strides=stc))\n",
    "    \n",
    "    if type_pooling == 'Max':\n",
    "        model.add(MaxPooling2D((ps, ps),padding=padding_type,strides=stp))\n",
    "    elif type_pooling == 'Average':\n",
    "        model.add(AveragePooling2D((ps, ps),padding=padding_type,strides=stp))\n",
    "        \n",
    "    model.add(Dropout(do))\n",
    "    if bn==True:\n",
    "        model.add(BatchNormalization())\n",
    "        \n",
    "    model.add(Conv2D(nfilters*2, (ks, ks), activation=activation_conv,padding=padding_type,strides=stc))\n",
    "    \n",
    "    if type_pooling == 'Max':\n",
    "        model.add(MaxPooling2D((ps, ps),padding=padding_type,strides=stp))\n",
    "    elif type_pooling == 'Average':\n",
    "        model.add(AveragePooling2D((ps, ps),padding=padding_type,strides=stp))\n",
    "        \n",
    "    model.add(SpatialDropout2D(do))\n",
    "    if bn==True:\n",
    "        model.add(BatchNormalization())\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(num_classes*md*md, activation=activation_conv))\n",
    "    model.add(Dense(num_classes*md, activation=activation_conv))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    model.compile(loss=keras.losses.categorical_crossentropy, \n",
    "        optimizer=keras.optimizers.Adam(lr=0.0001),metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe4a015-3ce8-4026-b3e9-30c64c475303",
   "metadata": {},
   "source": [
    "## 1. No weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "42a747fa-fb0e-41c4-ad5c-b888138d2936",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_models = '/glade/work/jhayron/Weather_Regimes/models/CNN/weights_variables_v8/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eedbf0d2-2949-4c79-823a-bb520e4aa20c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************************************************************************************\n",
      "z500\n",
      "********************************************************************************************\n",
      "week1\n",
      "{'activation': 'ReLU', 'bn': True, 'bs': 32, 'do': 0.3, 'ks': 7, 'md': 2, 'nfilters': 4, 'ps': 6, 'stc': 1, 'stp': 2, 'type_pooling': 'Max'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-16 13:02:03.020448: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2023-03-16 13:02:03.021893: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n",
      "2023-03-16 13:02:03.060900: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:3d:00.0 name: Tesla V100-SXM2-32GB computeCapability: 7.0\n",
      "coreClock: 1.53GHz coreCount: 80 deviceMemorySize: 31.75GiB deviceMemoryBandwidth: 836.37GiB/s\n",
      "2023-03-16 13:02:03.060967: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2023-03-16 13:02:03.266261: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n",
      "2023-03-16 13:02:03.266318: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.10\n",
      "2023-03-16 13:02:03.376101: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2023-03-16 13:02:03.573592: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2023-03-16 13:02:03.702365: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2023-03-16 13:02:03.802628: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.10\n",
      "2023-03-16 13:02:03.949858: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\n",
      "2023-03-16 13:02:03.951009: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n",
      "2023-03-16 13:02:03.953780: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-16 13:02:03.953875: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2023-03-16 13:02:03.954489: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:3d:00.0 name: Tesla V100-SXM2-32GB computeCapability: 7.0\n",
      "coreClock: 1.53GHz coreCount: 80 deviceMemorySize: 31.75GiB deviceMemoryBandwidth: 836.37GiB/s\n",
      "2023-03-16 13:02:03.954525: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2023-03-16 13:02:03.954542: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n",
      "2023-03-16 13:02:03.954555: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.10\n",
      "2023-03-16 13:02:03.954568: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2023-03-16 13:02:03.954581: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2023-03-16 13:02:03.954594: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2023-03-16 13:02:03.954607: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.10\n",
      "2023-03-16 13:02:03.954620: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\n",
      "2023-03-16 13:02:03.955586: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n",
      "2023-03-16 13:02:03.955642: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2023-03-16 13:02:07.204848: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2023-03-16 13:02:07.204889: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 \n",
      "2023-03-16 13:02:07.204909: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N \n",
      "2023-03-16 13:02:07.206715: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 30132 MB memory) -> physical GPU (device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:3d:00.0, compute capability: 7.0)\n",
      "2023-03-16 13:02:08.388704: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2023-03-16 13:02:08.389513: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2300000000 Hz\n",
      "2023-03-16 13:02:08.990228: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n",
      "2023-03-16 13:02:09.599909: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - 0s 6ms/step - loss: 0.5397 - accuracy: 0.8180\n",
      "loss/acc model\n",
      "[0.5396832823753357, 0.8180180191993713]\n",
      "loss/acc persistence\n",
      "[0.0, 1.0]\n",
      "week2\n",
      "{'activation': 'LeakyReLU', 'bn': False, 'bs': 32, 'do': 0.4, 'ks': 5, 'md': 4, 'nfilters': 4, 'ps': 4, 'stc': 3, 'stp': 1, 'type_pooling': 'Average'}\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 1.3646 - accuracy: 0.4396\n",
      "loss/acc model\n",
      "[1.3646208047866821, 0.4396396279335022]\n",
      "loss/acc persistence\n",
      "[20.661033809077633, 0.4018018018018018]\n",
      "week3\n",
      "{'activation': 'ReLU', 'bn': True, 'bs': 32, 'do': 0.3, 'ks': 7, 'md': 8, 'nfilters': 4, 'ps': 4, 'stc': 2, 'stp': 3, 'type_pooling': 'Max'}\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 1.4992 - accuracy: 0.2919\n",
      "loss/acc model\n",
      "[1.4991958141326904, 0.2918919026851654]\n",
      "loss/acc persistence\n",
      "[26.5108445863466, 0.23243243243243245]\n"
     ]
    }
   ],
   "source": [
    "dic_metrics = {}\n",
    "\n",
    "for var_short, variable in zip(name_var,variables):\n",
    "    print('********************************************************************************************')\n",
    "    print(variable)\n",
    "    print('********************************************************************************************')\n",
    "    loss_weeks_model = []\n",
    "    loss_weeks_persistence = []\n",
    "    acc_weeks_model = []\n",
    "    acc_weeks_persistence = []\n",
    "\n",
    "    for week in ['week1','week2','week3']:\n",
    "        print(week)\n",
    "        #### ORGANIZE DATA ####\n",
    "        week_output_wr = df_wr_2[week].values.astype(int)\n",
    "        # Make Y categorical\n",
    "        serie_wr_categorical = to_categorical(week_output_wr,num_classes=5)\n",
    "        week1_anoms = copy.deepcopy(dic_vars[variable])\n",
    "        \n",
    "        # # Scale by min-max\n",
    "        Min = week1_anoms[f'{var_short}_anomalies'].min(dim='time')\n",
    "        Max = week1_anoms[f'{var_short}_anomalies'].max(dim='time')\n",
    "        scaled_x = (week1_anoms[f'{var_short}_anomalies']) / (Max - Min)\n",
    "\n",
    "        indices = np.arange(len(serie_wr_categorical))\n",
    "        #Reshape X\n",
    "        scaled_x = scaled_x.data.reshape(-1, scaled_x.shape[1],scaled_x.shape[2], 1)\n",
    "\n",
    "        indices_train = np.where(df_wr_2.week2.index.year<=2001)[0]\n",
    "        indices_val = np.where((df_wr_2.week2.index.year>2001)&(df_wr_2.week2.index.year<=2010))[0]\n",
    "        indices_test = np.where(df_wr_2.week2.index.year>2010)[0]\n",
    "\n",
    "        X_test = scaled_x[indices_test]\n",
    "        y_test = serie_wr_categorical[indices_test]\n",
    "\n",
    "        X_train = scaled_x[indices_train]\n",
    "        y_train = serie_wr_categorical[indices_train]\n",
    "\n",
    "        X_val = scaled_x[indices_val]\n",
    "        y_val = serie_wr_categorical[indices_val]\n",
    "\n",
    "        wr_persistence = df_wr_2.week1.values.astype(int)[indices_test]\n",
    "        serie_wr_persistence_categorical = to_categorical(wr_persistence)\n",
    "        \n",
    "        y_train_integers = np.argmax(y_train, axis=1)\n",
    "#         class_weights = compute_class_weight(class_weight='balanced',classes=np.unique(y_train_integers),\n",
    "#                                              y = y_train_integers)\n",
    "#         d_class_weights = dict(enumerate(class_weights))\n",
    "        \n",
    "        ## TRAIN\n",
    "        results_directory = f'/glade/work/jhayron/Weather_Regimes/models/CNN/results_optuna/{week}/'\n",
    "        study_optuna = joblib.load(results_directory + 'optuna_study_v3.pkl')\n",
    "        dict_params = study_optuna.best_params\n",
    "        print(dict_params)\n",
    "        keras.backend.clear_session()\n",
    "\n",
    "        model = create_model(dict_params['ks'],\n",
    "                             dict_params['ps'],\n",
    "                             dict_params['type_pooling'],\n",
    "                             dict_params['stc'],\n",
    "                             dict_params['stp'],\n",
    "                             dict_params['do'],\n",
    "                             dict_params['bn'],\n",
    "                             dict_params['md'],\n",
    "                             dict_params['nfilters'],\n",
    "                             dict_params['activation'],\n",
    "                            )\n",
    "        \n",
    "        batch_size = dict_params['bs']\n",
    "        epochs = 200\n",
    "\n",
    "        earlystop = EarlyStopping(monitor='val_loss', patience=5)\n",
    "        try:\n",
    "            os.mkdir(f'{path_models}{variable}')\n",
    "        except: pass\n",
    "        filepath = f'{path_models}{variable}/model_{week}_v8.h5'\n",
    "        checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=0, save_best_only=True, \n",
    "                                     mode='auto',save_weights_only=False)\n",
    "        model.fit(X_train, y_train, batch_size=batch_size,\\\n",
    "                  epochs=epochs,verbose=0,validation_data=(X_val, y_val),\\\n",
    "                  callbacks=[checkpoint,earlystop])#,\n",
    "                  # class_weight = d_class_weights)\n",
    "\n",
    "        #### EVAL ####\n",
    "\n",
    "        model.load_weights(filepath)\n",
    "        model.save(filepath)\n",
    "        metrics_model = model.evaluate(x=X_test,y=y_test)\n",
    "        acc_temp = metrics_model[1]\n",
    "        loss_temp = metrics_model[0]\n",
    "        acc_persistence = accuracy_score(y_test,serie_wr_persistence_categorical)\n",
    "        loss_persistence = log_loss(y_test,serie_wr_persistence_categorical)\n",
    "        print('loss/acc model')\n",
    "        print(metrics_model)\n",
    "        print('loss/acc persistence')\n",
    "        print([loss_persistence,acc_persistence])\n",
    "\n",
    "        loss_weeks_model.append(loss_temp)\n",
    "        loss_weeks_persistence.append(loss_persistence)\n",
    "        acc_weeks_model.append(acc_temp)\n",
    "        acc_weeks_persistence.append(acc_persistence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0f9150-d042-4394-9502-328db1681780",
   "metadata": {},
   "source": [
    "## 2. Linear weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7b41fa50-0b10-44ec-977a-ad95201a5559",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_models = '/glade/work/jhayron/Weather_Regimes/models/CNN/weights_variables_v8_weights_1/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7486d995-dfaf-44d2-9aad-0b3ed2c834c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************************************************************************************\n",
      "z500\n",
      "********************************************************************************************\n",
      "week1\n",
      "{'activation': 'ReLU', 'bn': True, 'bs': 32, 'do': 0.3, 'ks': 7, 'md': 2, 'nfilters': 4, 'ps': 6, 'stc': 1, 'stp': 2, 'type_pooling': 'Max'}\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 0.6530 - accuracy: 0.7748\n",
      "loss/acc model\n",
      "[0.6529822945594788, 0.7747747898101807]\n",
      "loss/acc persistence\n",
      "[0.0, 1.0]\n",
      "week2\n",
      "{'activation': 'LeakyReLU', 'bn': False, 'bs': 32, 'do': 0.4, 'ks': 5, 'md': 4, 'nfilters': 4, 'ps': 4, 'stc': 3, 'stp': 1, 'type_pooling': 'Average'}\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 1.4171 - accuracy: 0.3982\n",
      "loss/acc model\n",
      "[1.4170533418655396, 0.3981981873512268]\n",
      "loss/acc persistence\n",
      "[20.661033809077633, 0.4018018018018018]\n",
      "week3\n",
      "{'activation': 'ReLU', 'bn': True, 'bs': 32, 'do': 0.3, 'ks': 7, 'md': 8, 'nfilters': 4, 'ps': 4, 'stc': 2, 'stp': 3, 'type_pooling': 'Max'}\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 1.5622 - accuracy: 0.2396\n",
      "loss/acc model\n",
      "[1.5621581077575684, 0.23963963985443115]\n",
      "loss/acc persistence\n",
      "[26.5108445863466, 0.23243243243243245]\n"
     ]
    }
   ],
   "source": [
    "dic_metrics = {}\n",
    "\n",
    "for var_short, variable in zip(name_var,variables):\n",
    "    print('********************************************************************************************')\n",
    "    print(variable)\n",
    "    print('********************************************************************************************')\n",
    "    loss_weeks_model = []\n",
    "    loss_weeks_persistence = []\n",
    "    acc_weeks_model = []\n",
    "    acc_weeks_persistence = []\n",
    "\n",
    "    for week in ['week1','week2','week3']:\n",
    "        print(week)\n",
    "        #### ORGANIZE DATA ####\n",
    "        week_output_wr = df_wr_2[week].values.astype(int)\n",
    "        # Make Y categorical\n",
    "        serie_wr_categorical = to_categorical(week_output_wr,num_classes=5)\n",
    "        week1_anoms = copy.deepcopy(dic_vars[variable])\n",
    "        \n",
    "        # # Scale by min-max\n",
    "        Min = week1_anoms[f'{var_short}_anomalies'].min(dim='time')\n",
    "        Max = week1_anoms[f'{var_short}_anomalies'].max(dim='time')\n",
    "        scaled_x = (week1_anoms[f'{var_short}_anomalies']) / (Max - Min)\n",
    "\n",
    "        indices = np.arange(len(serie_wr_categorical))\n",
    "        #Reshape X\n",
    "        scaled_x = scaled_x.data.reshape(-1, scaled_x.shape[1],scaled_x.shape[2], 1)\n",
    "\n",
    "        indices_train = np.where(df_wr_2.week2.index.year<=2001)[0]\n",
    "        indices_val = np.where((df_wr_2.week2.index.year>2001)&(df_wr_2.week2.index.year<=2010))[0]\n",
    "        indices_test = np.where(df_wr_2.week2.index.year>2010)[0]\n",
    "\n",
    "        X_test = scaled_x[indices_test]\n",
    "        y_test = serie_wr_categorical[indices_test]\n",
    "\n",
    "        X_train = scaled_x[indices_train]\n",
    "        y_train = serie_wr_categorical[indices_train]\n",
    "\n",
    "        X_val = scaled_x[indices_val]\n",
    "        y_val = serie_wr_categorical[indices_val]\n",
    "\n",
    "        wr_persistence = df_wr_2.week1.values.astype(int)[indices_test]\n",
    "        serie_wr_persistence_categorical = to_categorical(wr_persistence)\n",
    "        \n",
    "        y_train_integers = np.argmax(y_train, axis=1)\n",
    "        class_weights = compute_class_weight(class_weight='balanced',classes=np.unique(y_train_integers),\n",
    "                                             y = y_train_integers)\n",
    "        d_class_weights = dict(enumerate(class_weights))\n",
    "\n",
    "        ## TRAIN\n",
    "        results_directory = f'/glade/work/jhayron/Weather_Regimes/models/CNN/results_optuna/{week}/'\n",
    "        study_optuna = joblib.load(results_directory + 'optuna_study_v3.pkl')\n",
    "        dict_params = study_optuna.best_params\n",
    "        print(dict_params)\n",
    "        keras.backend.clear_session()\n",
    "\n",
    "        model = create_model(dict_params['ks'],\n",
    "                             dict_params['ps'],\n",
    "                             dict_params['type_pooling'],\n",
    "                             dict_params['stc'],\n",
    "                             dict_params['stp'],\n",
    "                             dict_params['do'],\n",
    "                             dict_params['bn'],\n",
    "                             dict_params['md'],\n",
    "                             dict_params['nfilters'],\n",
    "                             dict_params['activation'],\n",
    "                            )\n",
    "        \n",
    "        batch_size = dict_params['bs']\n",
    "        epochs = 200\n",
    "\n",
    "        earlystop = EarlyStopping(monitor='val_loss', patience=5)\n",
    "        try:\n",
    "            os.mkdir(f'{path_models}{variable}')\n",
    "        except: pass\n",
    "        filepath = f'{path_models}{variable}/model_{week}_v8.h5'\n",
    "        checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=0, save_best_only=True, \n",
    "                                     mode='auto',save_weights_only=False)\n",
    "        model.fit(X_train, y_train, batch_size=batch_size,\\\n",
    "                  epochs=epochs,verbose=0,validation_data=(X_val, y_val),\\\n",
    "                  callbacks=[checkpoint,earlystop],\n",
    "                  class_weight = d_class_weights)\n",
    "\n",
    "        #### EVAL ####\n",
    "\n",
    "        model.load_weights(filepath)\n",
    "        model.save(filepath)\n",
    "        metrics_model = model.evaluate(x=X_test,y=y_test)\n",
    "        acc_temp = metrics_model[1]\n",
    "        loss_temp = metrics_model[0]\n",
    "        acc_persistence = accuracy_score(y_test,serie_wr_persistence_categorical)\n",
    "        loss_persistence = log_loss(y_test,serie_wr_persistence_categorical)\n",
    "        print('loss/acc model')\n",
    "        print(metrics_model)\n",
    "        print('loss/acc persistence')\n",
    "        print([loss_persistence,acc_persistence])\n",
    "\n",
    "        loss_weeks_model.append(loss_temp)\n",
    "        loss_weeks_persistence.append(loss_persistence)\n",
    "        acc_weeks_model.append(acc_temp)\n",
    "        acc_weeks_persistence.append(acc_persistence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747a85b5-ffe1-416f-8064-6e049787c6ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cnn_wr]",
   "language": "python",
   "name": "conda-env-cnn_wr-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
