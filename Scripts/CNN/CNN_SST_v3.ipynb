{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f074931-56b1-4d97-abf2-dc600525ddb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import glob\n",
    "import pickle\n",
    "import warnings\n",
    "from datetime import datetime, timedelta\n",
    "from itertools import product\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "import cartopy\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature\n",
    "import cartopy.feature as cfeature\n",
    "import cartopy.feature as cf\n",
    "import cartopy.io.shapereader as shpreader\n",
    "import matplotlib as mpl\n",
    "import matplotlib.path as mpath\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "# np.random.seed(42)\n",
    "import pandas as pd\n",
    "import shapely.geometry as sgeom\n",
    "import xarray as xr\n",
    "from scipy import stats\n",
    "from scipy.spatial.distance import cdist\n",
    "from shapely import geometry\n",
    "from sklearn import metrics\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# import cluster_analysis, narm_analysis, som_analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73da05ac-ef73-41f3-9efd-7f1743760936",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90aee4bc-9be4-458e-b3c4-f637b16a7016",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Weather regimes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a40047a6-fca9-4719-9b9c-0d9c0b814f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "week1_wr = pd.read_csv('/glade/work/jhayron/Weather_Regimes/weekly_wr/week1_wr_v3.csv',\\\n",
    "                      index_col = 0, parse_dates = True)\n",
    "week2_wr = pd.read_csv('/glade/work/jhayron/Weather_Regimes/weekly_wr/week2_wr_v3.csv',\\\n",
    "                      index_col = 0, parse_dates = True)\n",
    "week3_wr = pd.read_csv('/glade/work/jhayron/Weather_Regimes/weekly_wr/week3_wr_v3.csv',\\\n",
    "                      index_col = 0, parse_dates = True)\n",
    "week4_wr = pd.read_csv('/glade/work/jhayron/Weather_Regimes/weekly_wr/week4_wr_v3.csv',\\\n",
    "                      index_col = 0, parse_dates = True)\n",
    "week5_wr = pd.read_csv('/glade/work/jhayron/Weather_Regimes/weekly_wr/week5_wr_v3.csv',\\\n",
    "                      index_col = 0, parse_dates = True)\n",
    "week6_wr = pd.read_csv('/glade/work/jhayron/Weather_Regimes/weekly_wr/week6_wr_v3.csv',\\\n",
    "                      index_col = 0, parse_dates = True)\n",
    "week7_wr = pd.read_csv('/glade/work/jhayron/Weather_Regimes/weekly_wr/week7_wr_v3.csv',\\\n",
    "                      index_col = 0, parse_dates = True)\n",
    "week8_wr = pd.read_csv('/glade/work/jhayron/Weather_Regimes/weekly_wr/week8_wr_v3.csv',\\\n",
    "                      index_col = 0, parse_dates = True)\n",
    "week9_wr = pd.read_csv('/glade/work/jhayron/Weather_Regimes/weekly_wr/week9_wr_v3.csv',\\\n",
    "                      index_col = 0, parse_dates = True)\n",
    "\n",
    "df_wr = pd.concat([week1_wr,week2_wr,week3_wr,week4_wr,week5_wr,week6_wr,week7_wr,week8_wr,week9_wr],axis=1)\n",
    "df_wr.columns = ['week1','week2','week3','week4','week5','week6','week7','week8','week9']\n",
    "\n",
    "df_wr_2 = pd.read_csv('/glade/work/jhayron/Weather_Regimes/weekly_wr/weekly_wr_mean_geop_v3.csv',\n",
    "                     index_col=0,parse_dates=True)\n",
    "\n",
    "df_wr_2 = df_wr_2.dropna()\n",
    "df_wr = df_wr.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e404c3d-4968-4204-b0ed-fe95bfb6d6ae",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Input Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "beb3164d-10c0-4073-ac3a-4f8d4634b5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "variables = ['sst']\n",
    "name_var = ['sst']\n",
    "units = ['K']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "caa0c0e0-15fa-47a6-8117-d6f2a8313107",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dic_vars = {}\n",
    "for var_short, variable,unit in zip(name_var,variables,units):\n",
    "# for var_short, variable,unit in zip(['sst'],['sst'],['K']):\n",
    "    path_w_anoms = '/glade/work/jhayron/Weather_Regimes/weekly_anomalies/'\n",
    "    week1_anoms = xr.open_dataset(f'{path_w_anoms}week1_{variable}_anoms_v3.nc')\n",
    "    # week1_anoms = week1_anoms.sel(time=df_wr_2.index)\n",
    "    if variable=='z500':\n",
    "        week1_anoms = week1_anoms.where(week1_anoms.lat>-30,drop=True)\n",
    "    # week1_anoms = week1_anoms.sel(time=df_wr.index)\n",
    "    week1_anoms = week1_anoms.sel(time=df_wr_2.index)\n",
    "    dic_vars[variable] = week1_anoms\n",
    "    \n",
    "    ##########PLOT#####################\n",
    "#     fig = plt.figure(figsize=(9,7))\n",
    "#     ax = fig.add_subplot(111,projection=ccrs.PlateCarree(central_longitude=-90+360))\n",
    "    \n",
    "#     #     vmax = np.round(np.percentile(abs(week1_anoms[f'{var_short}_anomalies'].values[0]),100),0)\n",
    "#     #     vmin = -np.round(np.percentile(abs(week1_anoms[f'{var_short}_anomalies'].values[0]),100),0)\n",
    "    \n",
    "#     vmax = np.round(np.percentile(abs(week1_anoms[f'{var_short}_anomalies'].values[0]),100),0)\n",
    "#     vmin = -np.round(np.percentile(abs(week1_anoms[f'{var_short}_anomalies'].values[0]),100),0)\n",
    "#     # print(vmax,vmin)\n",
    "#     cs = plt.pcolormesh(week1_anoms.lon,week1_anoms.lat,\\\n",
    "#         week1_anoms[f'{var_short}_anomalies'].values[0],cmap='seismic',\n",
    "#         transform=ccrs.PlateCarree(),vmin=vmin,vmax=vmax)\n",
    "#     ax.coastlines(resolution='110m', color='k', linewidth=0.75, zorder=10)\n",
    "#     ax.margins(x=0, y=0)\n",
    "\n",
    "#     # ax.set_extent([-179, 179, 10, 90], crs=ccrs.PlateCarree())\n",
    "#     if variable!='st':\n",
    "#         plt.title(variable.upper().replace('_','-'))\n",
    "#     else:\n",
    "#         plt.title(variable.upper().replace('_','-').replace('ST','TS'))\n",
    "#     if 'region' in variable:\n",
    "#         cbar_ax = fig.add_axes([0.25, 0.19, 0.5, 0.0175])\n",
    "#     else:\n",
    "#         cbar_ax = fig.add_axes([0.25, 0.3, 0.5, 0.0175])\n",
    "#     # ticks_1 = [-80, -40, 0, 40, 80]\n",
    "#     cbar = fig.colorbar(cs, cax=cbar_ax,\n",
    "#                         orientation='horizontal', extend='both')\n",
    "#     cbar.ax.tick_params(labelsize=14)\n",
    "#     cbar.set_label(unit, fontsize=14)\n",
    "#     plt.savefig(f'/glade/u/home/jhayron/WeatherRegimes/Figures/MapsVariables/{variable}_anomalies_v2.png',bbox_inches='tight')\n",
    "#     plt.close('all')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b09832-3831-4dc7-aeb2-6b7b65e854e8",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Build models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ddc9b5e7-a5ad-44d1-8c4e-34456f6484ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential,Model\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, AveragePooling2D\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.layers import ReLU\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import accuracy_score\n",
    "# import visualkeras\n",
    "# import tensorflow as tf\n",
    "\n",
    "# ## GLOBAL SEED ##    \n",
    "# np.random.seed(42)\n",
    "# tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472b482e-df08-4a1a-8d3d-39e9d75a8717",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f2aafbf6-aa12-479e-bc0e-f6896f991198",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(activation_conv = ReLU()):\n",
    "    num_classes = 4\n",
    "    ks = 3\n",
    "    ps = 5\n",
    "    nfilters=64\n",
    "    padding_type = 'valid'\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(nfilters, kernel_size=(ks, ks),activation=activation_conv,\n",
    "        input_shape=X_train.shape[1:],padding=padding_type))\n",
    "    model.add(AveragePooling2D((ps, ps),padding=padding_type))\n",
    "    \n",
    "    model.add(Conv2D(nfilters*2, (ks, ks), activation=activation_conv,padding=padding_type))\n",
    "    model.add(AveragePooling2D(pool_size=(ps, ps),padding=padding_type))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(nfilters*2, activation=activation_conv))                \n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    model.compile(loss=keras.losses.categorical_crossentropy, \n",
    "        optimizer=keras.optimizers.Adam(lr=0.0001),metrics=['accuracy'])\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5daa2338-fc25-4638-b503-36e7da6b6ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_models = '/glade/work/jhayron/Weather_Regimes/models/CNN/weights_variables_v3/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b1c24ab-06d0-42c5-91a2-fa4a70e28111",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************************************************************************************\n",
      "sst\n",
      "********************************************************************************************\n",
      "week2\n",
      "WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0041s vs `on_train_batch_end` time: 0.0263s). Check your callbacks.\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 1.3936 - accuracy: 0.2973\n",
      "loss/acc model\n",
      "[1.3935846090316772, 0.29729729890823364]\n",
      "loss/acc persistence\n",
      "[19.78978539544183, 0.42702702702702705]\n",
      "week3\n",
      "WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0028s vs `on_train_batch_end` time: 0.0240s). Check your callbacks.\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 1.3916 - accuracy: 0.3063\n",
      "loss/acc model\n",
      "[1.3915660381317139, 0.30630630254745483]\n",
      "loss/acc persistence\n",
      "[25.701828202256213, 0.25585585585585585]\n",
      "week4\n",
      "WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0031s vs `on_train_batch_end` time: 0.0223s). Check your callbacks.\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 1.3896 - accuracy: 0.3045\n",
      "loss/acc model\n",
      "[1.3895567655563354, 0.30450451374053955]\n",
      "loss/acc persistence\n",
      "[25.26620399543831, 0.26846846846846845]\n",
      "week5\n",
      "WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0029s vs `on_train_batch_end` time: 0.0245s). Check your callbacks.\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 1.3811 - accuracy: 0.3153\n",
      "loss/acc model\n",
      "[1.3810914754867554, 0.315315306186676]\n",
      "loss/acc persistence\n",
      "[24.33272355225709, 0.2954954954954955]\n",
      "week6\n",
      "WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0043s vs `on_train_batch_end` time: 0.0116s). Check your callbacks.\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 1.3793 - accuracy: 0.3081\n",
      "loss/acc model\n",
      "[1.3792682886123657, 0.3081081211566925]\n",
      "loss/acc persistence\n",
      "[24.208259493166263, 0.2990990990990991]\n",
      "week7\n",
      "WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0033s vs `on_train_batch_end` time: 0.0238s). Check your callbacks.\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 1.3797 - accuracy: 0.2991\n",
      "loss/acc model\n",
      "[1.3797006607055664, 0.2990990877151489]\n",
      "loss/acc persistence\n",
      "[24.208259493166263, 0.2990990990990991]\n",
      "week8\n",
      "WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0027s vs `on_train_batch_end` time: 0.0221s). Check your callbacks.\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 1.3863 - accuracy: 0.3009\n",
      "loss/acc model\n",
      "[1.3863095045089722, 0.3009009063243866]\n",
      "loss/acc persistence\n",
      "[24.33272355225709, 0.2954954954954955]\n",
      "week9\n",
      "WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0031s vs `on_train_batch_end` time: 0.0318s). Check your callbacks.\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 1.3878 - accuracy: 0.3027\n",
      "loss/acc model\n",
      "[1.387832522392273, 0.3027026951313019]\n",
      "loss/acc persistence\n",
      "[24.955043847711238, 0.2774774774774775]\n"
     ]
    }
   ],
   "source": [
    "dic_metrics = {}\n",
    "\n",
    "for var_short, variable in zip(name_var,variables):\n",
    "    print('********************************************************************************************')\n",
    "    print(variable)\n",
    "    print('********************************************************************************************')\n",
    "    loss_weeks_model = []\n",
    "    loss_weeks_persistence = []\n",
    "    acc_weeks_model = []\n",
    "    acc_weeks_persistence = []\n",
    "\n",
    "    for week in ['week2','week3','week4','week5','week6','week7','week8','week9']:\n",
    "        print(week)\n",
    "        #### ORGANIZE DATA ####\n",
    "        week_output_wr = df_wr_2[week].values.astype(int)\n",
    "        # Make Y categorical\n",
    "        serie_wr_categorical = to_categorical(week_output_wr,num_classes=4)\n",
    "        \n",
    "        week1_anoms = copy.deepcopy(dic_vars[variable])\n",
    "        \n",
    "        # # Scale by min-max\n",
    "        where = np.where(pd.to_datetime(week1_anoms.time.values).year<=2010)[0]\n",
    "        Min = week1_anoms.isel(time=where)[f'{var_short}_anomalies'].min(dim='time')\n",
    "        Max = week1_anoms.isel(time=where)[f'{var_short}_anomalies'].max(dim='time')\n",
    "        scaled_x = (week1_anoms[f'{var_short}_anomalies']) / (Max - Min)\n",
    "\n",
    "        indices = np.arange(len(serie_wr_categorical))\n",
    "        #Reshape X\n",
    "        scaled_x = scaled_x.data.reshape(-1, scaled_x.shape[1],scaled_x.shape[2], 1)\n",
    "        scaled_x[np.isfinite(scaled_x)==False]=0\n",
    "        indices_train = np.where(df_wr_2.week2.index.year<=2001)[0]\n",
    "        indices_val = np.where((df_wr_2.week2.index.year>2001)&(df_wr_2.week2.index.year<=2010))[0]\n",
    "        indices_test = np.where(df_wr_2.week2.index.year>2010)[0]\n",
    "\n",
    "        X_test = scaled_x[indices_test]\n",
    "        y_test = serie_wr_categorical[indices_test]\n",
    "\n",
    "        X_train = scaled_x[indices_train]\n",
    "        y_train = serie_wr_categorical[indices_train]\n",
    "\n",
    "        X_val = scaled_x[indices_val]\n",
    "        y_val = serie_wr_categorical[indices_val]\n",
    "\n",
    "        wr_persistence = df_wr_2.week1.values.astype(int)[indices_test]\n",
    "        serie_wr_persistence_categorical = to_categorical(wr_persistence)\n",
    "\n",
    "        #### TRAIN ####\n",
    "\n",
    "        keras.backend.clear_session()\n",
    "        model = create_model()\n",
    "        batch_size = 16\n",
    "        epochs = 200\n",
    "\n",
    "        earlystop = EarlyStopping(monitor='val_loss', patience=5)\n",
    "        try:\n",
    "            os.mkdir(f'{path_models}{variable}')\n",
    "        except: pass\n",
    "        filepath = f'{path_models}{variable}/model_{week}_v3.h5'\n",
    "        checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=0, save_best_only=True, \n",
    "                                     mode='auto',save_weights_only=False)\n",
    "        model.fit(X_train, y_train, batch_size=batch_size,\\\n",
    "            epochs=epochs,verbose=0,validation_data=(X_val, y_val), callbacks=[checkpoint,earlystop])\n",
    "\n",
    "        #### EVAL ####\n",
    "\n",
    "        model.load_weights(filepath)\n",
    "        # model.save(filepath) WHAT IS THIS\n",
    "        metrics_model = model.evaluate(x=X_test,y=y_test)\n",
    "        acc_temp = metrics_model[1]\n",
    "        loss_temp = metrics_model[0]\n",
    "        acc_persistence = accuracy_score(y_test,serie_wr_persistence_categorical)\n",
    "        loss_persistence = log_loss(y_test,serie_wr_persistence_categorical)\n",
    "        print('loss/acc model')\n",
    "        print(metrics_model)\n",
    "        print('loss/acc persistence')\n",
    "        print([loss_persistence,acc_persistence])\n",
    "\n",
    "        loss_weeks_model.append(loss_temp)\n",
    "        loss_weeks_persistence.append(loss_persistence)\n",
    "        acc_weeks_model.append(acc_temp)\n",
    "        acc_weeks_persistence.append(acc_persistence)\n",
    "        \n",
    "    ### PLOT BARS ###\n",
    "    plt.bar(np.arange(8),acc_weeks_model,width=-0.3,align='edge',label=f'CNN - {variable.upper()}',color='k')\n",
    "    plt.bar(np.arange(8),acc_weeks_persistence,width=0.3,align='edge',label='Persistence',color='darkorange')\n",
    "    plt.legend()\n",
    "    plt.ylabel('Percentage of accurate predictions [%]\\nTesting Sample (15%)')\n",
    "    plt.xticks(np.arange(8),['Week 1','Week 2','Week 3','Week 4','Week 5','Week 6','Week 7','Week 8'])\n",
    "    plt.xlabel('Lead time')\n",
    "    path_figures = '/glade/u/home/jhayron/WeatherRegimes/Figures/Variables/'\n",
    "    plt.savefig(f'{path_figures}{variable}_SvP_v2_test.png',bbox_inches='tight')\n",
    "    plt.close('all')\n",
    "    \n",
    "    ### SAVE METRICS ###\n",
    "    \n",
    "    df_metrics = pd.DataFrame(np.array([acc_weeks_model,acc_weeks_persistence,loss_weeks_model,loss_weeks_persistence]).T,\n",
    "            columns = ['acc_model','acc_persistence','loss_model','loss_persistence'])\n",
    "    dic_metrics[variable] = df_metrics\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NPL-3.7.9",
   "language": "python",
   "name": "npl-3.7.9"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
